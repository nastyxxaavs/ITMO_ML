# -*- coding: utf-8 -*-
"""ML_lab1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tAGPb9K4UyJuk8LYppKfmnq7aPxMpp8a

Установка необходимых библиотек
"""

!pip3 install -U pandas
import pandas as pd
!pip3 install requests
import multiprocessing
import requests
!pip install bs4
import bs4 as bs
!pip install xlsxwriter
import xlsxwriter
import re
import csv
from datetime import datetime, timedelta

"""Парсинг сайта https://www.film.ru/compilation"""

main_url='https://www.film.ru/compilation/page/'
cur_url = 'https://www.film.ru/compilation/'
headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:130.0) Gecko/20100101 Firefox/130.0'}
data = [['Категория', 'Страна', 'Оценка зрителей', 'Оценка сайта', 'Оценка IMDb', 'Год выхода']]

def get_soup(url):
  try:
        result = requests.get(url, headers)
        result.raise_for_status()
        if not result.text:
            print("Страница пустая.")
            return None
        return bs.BeautifulSoup(result.text, 'html.parser')

  except requests.exceptions.HTTPError as http_err:
        print(f"HTTP ошибка: {http_err}")
  except requests.exceptions.ConnectionError:
        print("Ошибка соединения: не удалось подключиться к серверу.")
  except requests.exceptions.Timeout:
        print("Ошибка таймаута: запрос превысил время ожидания.")
  except requests.exceptions.RequestException as req_err:
        print(f"Ошибка запроса: {req_err}")

  return None


def clean_rating(rating):
    return '0' if rating.strip() == '' else rating

def save_if_lowercase(string):
    if string and string[0].islower():
        return string
    return None


for page in range (0,4):
  newUrl = main_url+f"{page}"
  categories_page = get_soup(newUrl)
  page = page+1
  categories = categories_page.find_all('div', class_='wrapper_movies_compilations')
  for count in range(0,36):
    cat_link = categories[0].find_all('a')[count].attrs
    count=count+1
    string = ""
    for key,item in cat_link.items():
        string = item
        refac_string = string[12:]
        films_page = get_soup(cur_url+refac_string)
        strr = cur_url+refac_string
        films = films_page.find_all('div', class_='redesign_afisha_movies')
        tags = films_page.find_all('div', class_='redesign_afisha_movie')
        count = len(tags)
        for film in films:
          for i in range (0,count):
            category = film.find_all('div', class_='redesign_afisha_movie_main_info')[i].text
            pos = category.find('/')
            part_category = category[:pos]
            part_category = save_if_lowercase(part_category)
            part_country = category[pos + 2:]
            rating = film.find_all('div', class_='redesign_afisha_movie_main_rating')[i].text
            rate_pos1= rating.find('film.ru:')
            rate_pos2= rating.find('зрители:')
            rate_pos3= rating.find('IMDb:')
            viewer_rating = rating[rate_pos2 + 8:rate_pos3]
            site_rating = rating[rate_pos1 + 8:rate_pos2]
            imdb_rating = rating[rate_pos3 + 5:]
            date = film.find_all('div', class_='redesign_afisha_movie_main_subtitle')[i].text
            year = None
            years = re.findall(r'\b\d{4}\b', date)
            year = years[0]
            viewer_rating = clean_rating(viewer_rating)
            site_rating = clean_rating(site_rating)
            imdb_rating = clean_rating(imdb_rating)


            if part_category != None:
              data.append([part_category, part_country, viewer_rating, site_rating, imdb_rating, year])
              print([part_category, part_country, viewer_rating, site_rating, imdb_rating, year])

#Код дял сохранения результата парсинга в виде excel-таблицы
#with xlsxwriter.Workbook('films.xlsx') as workbook:
    #worksheet = workbook.add_worksheet()

    #for row_num, info in enumerate(data):
      #worksheet.write_row(row_num, 0, info)

with open('films.csv', mode='w', newline='', encoding='utf-8') as csvfile:
    writer = csv.writer(csvfile)

    for row in data:
        writer.writerow(row)

!pip install aspose-cells
import jpype
import asposecells
jpype.startJVM()
from asposecells.api import Workbook, SaveFormat

# Create a Workbook object with Excel file's path
workbook =  Workbook("/content/films.xlsx")

# Save XLSX as CSV
workbook.save("ExcelToCSV.csv" , SaveFormat.CSV)